---
title: "Homework 5"
author: "Mukta Patwari"
date: "2025-10-30"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(rvest)
library(readr)

knitr::opts_chunk$set(
  fig.path = "plots/",
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Problem 1

**Write a function that, for a fixed group size, randomly draws “birthdays” for each person; checks whether there are duplicate birthdays in the group; and returns TRUE or FALSE based on the result.**

```{r}
bday_sim = function(n_room) {
    birthdays = sample(1:365, n_room, replace = TRUE)
    
    repeated_bday = length(unique(birthdays)) < n_room
    
    repeated_bday
  }
```

**Next, run this function 10000 times for each group size between 2 and 50. For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs.**

```{r}
bday_sim_results =
  expand_grid(
    bdays = 2:50,
    iter = 1:10000
  ) %>%
  mutate(
    results = map_lgl(bdays, bday_sim)
  ) %>% 
  group_by(
    bdays
  ) %>% 
  summarize(
    prob_repeat = mean(results)
  )
```

**Make a plot showing the probability as a function of group size, and comment on your results.**

```{r}
bday_sim_results %>% 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_smooth(se = FALSE) +
  geom_point() +
  labs(
    x = "Number of people",
    y = "Probability of repeated birthdays"
  )
```

The plot shows that as sample size increases, the probability of repeated birthdays also increases. At around 23 people, the probability of repeated birthdays is 50%, and at around 40 people the probability of repeated birthdays is 87.5%. By the time the sample size is 50, the probability is close to 100%.

## Problem 2

```{r}
library(broom)

sim_t_test = function(n_subj = 30, sigma = 5, mu = 0) {
  
  sim_df =
    tibble(
      x = rnorm(n = n_subj, mean = mu, sd = sigma)
    ) %>% 
    summarize(
      tidy(t.test(x, mu = 0))
      ) %>% 
  mutate(
    hypothesis_test = case_when(
      p.value < 0.05 ~ "Reject",
      p.value > 0.05 ~ "Fail to Reject"
    )) %>% 
  select(estimate, p.value, hypothesis_test)
  }


sim_results_df =
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) %>% 
  mutate(
    results = map2(mu, iter, ~ sim_t_test(mu = .x))) %>% 
  unnest(results)
```

**Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis. Describe the association between effect size and power.**

```{r}
power_plot =
  sim_results_df %>% 
  group_by(mu) %>% 
  summarize(
    power = mean(hypothesis_test == "Reject"),
  ) %>%
  ungroup() %>% 
  ggplot(aes(x = mu, y = power)) +
  geom_smooth(se = FALSE) +
  geom_point() +
  scale_x_continuous(
    breaks = 0:6
  ) +
  labs(
    x = "True value of mu",
    y = "Power",
  )

print(power_plot)
```

The plot shows that there is a positive association between the true value of mu and the power of the test. When mu is around 0 (very small), the power is 0.05; we are using an alpha level of 0.05 when testing the null hypothesis that mu = 0 and so this hypothesis is barely rejected. As mu increases, power increases which shows that the probability of rejecting the null hypothesis increases for larger effect sizes.

**Make a plot showing the average estimate of mu-hat on the y axis and the true value of mu on the x axis. Make a second plot (or overlay on the first) the average estimate of mu-hat only in samples for which the null was rejected on the y axis and the true value of mu on the x axis.** 

```{r}
library(patchwork)

mu_hat_plot =
  sim_results_df %>% 
  group_by(mu) %>%
  summarize(
    mean_estimate = mean(estimate)
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x = mu, y = mean_estimate)) +
  geom_point() +
  geom_line(color = "red") +
  scale_x_continuous(
    breaks = 0:6
  ) +
  labs(
    x = "True value of mu",
    y = "Average estimate of mu hat"
  )

mu_hat_reject_plot =
  sim_results_df %>% 
  group_by(mu) %>% 
  summarize(
    mean_estimate_rejected = mean(estimate[hypothesis_test == "Reject"])
  ) %>% 
  ungroup() %>% 
  ggplot(aes(x = mu, y = mean_estimate_rejected)) +
  geom_point() +
  geom_line(color = "blue") +
  scale_x_continuous(
    breaks = 0:6
  ) +
  labs(
    x = "True value of mu",
    y = "Average estimate of rejected mu hat"
  )

mu_hat_plot + mu_hat_reject_plot
```

**Is the sample average of mu hat across tests for which the null is rejected approximately equal to the true value of mu? Why or why not**

No, the sample average of mu hat for tests where the null hypothesis was rejected is not approximately equal o the true mu value. When comparing the true value of mu and the average estimates of mu hat, we see a 1:1 line, which illustrates that the average estimate of mu hat is unbiased. However, when conditioning on the rejected mu hat estimates, the association appears to be biased. Plotting only the rejected mu hat estimates removes instances where mu hat is close to mu, and gives us the more extreme mu hat values. Accordingly, the line is biased. For smaller values of mu, less rejections of the null hypothesis are occurring and the estimated mu hat is slightly larger than the true value. As the true value of mu increases, more rejections of the null hypothesis occur and this becomes more consistent with the true values, but there is still bias due to chance.

## Problem 3

**Describe the raw data.**

```{r}
homicide_data =
  read_csv("homicide-data.csv") %>% 
  janitor::clean_names() 
```

The raw data contains information on homicides in various cities across the United States. There are `r nrow(homicide_data)` observations in the dataset, and the following variables `r colnames(homicide_data)`. The key variable is disposition, which has 3 categories: "Closed by arrest", "Closed without arrest", and "Open/No arrest". The latter two categories are considered to be unsolved homicides.

**Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).**

```{r}
homicide_data =
  homicide_data %>% 
  mutate(
    city_state = str_c(city, state, sep = ", ")
  ) %>% 
  group_by(city_state) %>% 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```

**For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of `prop.test` as an R object, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.**

```{r}
homicide_data %>% 
  group_by(city_state) %>% 
  filter(city_state == "Baltimore, MD") %>% 
  summarize(
  baltimore_prop = list(prop.test(unsolved_homicides, total_homicides))
  ) %>% 
  mutate(results = map(baltimore_prop, broom::tidy)) %>% 
  unnest(results) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  knitr::kable(digits = 3)
```

**Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city. Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.**

```{r}
library(purrr)

homicide_data %>% 
  group_by(city_state) %>% 
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, ~ prop.test(.x, .y)),
    results = map(prop_test, broom::tidy)
  ) %>%
  ungroup() %>% 
  unnest(results) %>%
  select(city_state, estimate, conf.low, conf.high) %>% 
  arrange(estimate) %>% 
  mutate(
    city_state = fct_inorder(city_state)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Proportion of Unsolved Homicides per City",
    x = "City",
    y = "Proportion (with 95% CI)"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 1))
```